prepared_dataset
================

.. py:module:: prepared_dataset


Attributes
----------

.. autoapisummary::

   prepared_dataset.__author__
   prepared_dataset.__description__
   prepared_dataset.logger


Classes
-------

.. autoapisummary::

   prepared_dataset.PreparedDataset


Module Contents
---------------

.. py:data:: __author__
   :value: 'tiantheunissen@gmail.com'


.. py:data:: __description__
   :value: 'Contains the PreparedDataset class for Knowit.'


   ------------------
   PreparedDataset
   ------------------

   The ``PreparedDataset`` represents a BaseDataset that is preprocessed for model training.
   It inherits from BaseDataset. Based on the provided `name' variable, it will populate the 
   parent BaseDataset's variables. 

   --------------------
   Prediction points
   --------------------

   In order to define a PreparedDataset, the input-output dynamics of the model to be trained 
   must defined rather precisely. The PreparedDataset is built on the idea of 'prediction points'.
   Each time step in the BaseDataset can be regarded a prediction point (under some assumptions). 
   At every prediction point, a model is to predict a specific set of features-over-time from 
   a specific other set of features-over-time. The specifics are defined as follows.

       -   in_components (list): A subset of the 'components' variable of the BaseDataset.
               These are the components that will be used as input to the model.
       -   out_components (list): A subset of the 'components' variable of the BaseDataset.
               These are the components that will be used as output to the model.
       -   in_chunk (list): A list of two integers [a, b] for which a <= b. This defines the 
               time steps (of in_components) to be used for prediction at point 
               t as [t + a, t + b].
       -   out_chunk (list): A list of two integers [a, b] for which a <= b. This defines the 
               time steps (of out_components) to be predicted at prediction point 
               t as [t + a, t + b].
               
   Note that this might seem cumbersome, but it allows us to easily define many different types of tasks:
       
       -   regression (heartrate from 11 millisecond window given three instantaneous biometrics)
               in_components = [biometric1, biometric2, biometric2]
               out_components = [heart rate]
               in_chunk = [-5, 5]
               out_chunk = [0, 0]
       - autoregressive univariate forcasting (predict a stock's value in 5 days given last 20 days)
               in_components = [stock,]
               out_components = [stock,]
               in_chunk = [-20, 0]
               out_chunk = [5, 5]
       - time series classification (detect a whale call given an audio recording of 5 seconds)
               in_components = [sound,]
               out_components = [positive_call,]
               in_chunk = [-2, 2]
               out_chunk = [0, 0]


   --------------------
   Input arguments
   --------------------

   In addition to the dataset name and prediction point dynamics defined above, the following variables 
   should also be provided.

   - name (str): The name of the new dataset option.
       - split_portions (tuple): The approximate portions of the (train, valid, eval) splits.
           Note these portions are considered in combination with the split_method.
       - seed (int): The seed for reproducibility.
       - batch_size (int): The mini-batch size for training.
       
       - shuffle_train (bool, optional): Whether the training set is shuffled after every epoch.
           Default: True
       - split_method (str, optional): The method of splitting data. See below for details.
           Default: 'chronological'
       - limit (int, optional): The number of instances / slices / time steps to limit 
           the data to. This depends on the split_method.
           Default: None = no limit.
       - scaling_method (str, optional): What method to use for scaling the data features. 
           See below for details. Default: z-norm
       - scaling_tag (str, optional): In what mode to scale the data. (in_only, full, None)
           See below for details. Default: None
       - padding_method (str, optional): What method to pad model inputs will.
           See below for details. Default: zero
       - min_slice (str, optional): The minimum slice size to consider 
           during data splitting / selection. Default: None = Consider all slices


   -----------------------
   Splitting & Limiting
   -----------------------

   The first step in preparing the dataset is to split it into a train-, validation-, 
   and evaluation set (train, valid, eval) along with limiting it if applicable. 
   This is done with the DataSplitter module.  More details can be found there, 
   but we summarize the options here:
   - `split_method` = 
       - 'random': Ignore all distinction between instances and slices, 
               and split on time steps randomly.
       - 'chronological' (default): Ignore all distinction between instances and slices, 
               and split on time steps chronologically.
       - 'slice-random': Ignore all distinction between instances, 
               and split on slices randomly.
       - 'slice-chronological': Ignore all distinction between instances, 
               and split on slices chronologically.
       - 'instance-random': Split on instances randomly.
       - 'instance-chronological': Split on instances chronologically.
   Note that the data is split ON the relevant level (instance, slice, or timesteps). 
   I.e. If you split on instances and there are only 3 instances, then split_portions=(0.6, 0.2, 0.2) 
   will be a wild approximation i.t.o actual time steps.
       
   Note that the data is limited during splitting, and the data is limited by removing 
   the excess data points from the end of the data block after shuffling or ordering according to time. 
   Also note that if the data is limited too much for a given split_portion to have a single entry,
    an error will occur confirming it.


   ----------
   Scaling
   ----------

   After the data is split and limited, a scaler is fit to the train set data 
   which will be applied to all data being extracted for model training.
   This is done with the DataScaler module. 
   More details can be found there, but we summarize the options here:
   - `scaling_method`
       - 'z-norm': Features are scaled by subtracting the mean and dividing by the std.
       - 'zero-one': Input features are scaled linearly to be in the range (0, 1).
       - None: No scaling occurs.
   If the task is regression, then the scaling also applies to the output features.
   - `scaling_tag`
       - 'in_only': Only the input features will be scaled.
       - 'full': The in and output features will be scaled.
       - None: No features will be scaled.


   ----------
   Padding
   ----------
   At some prediction points the `in_chunk' or 'out_chunk' might exceed the corresponding 
   slice range. In these cases we pad the input values. The output values are never padded.
   Prediction points that do not have valid output values for an 'out_chunk' are not selected 
   during data splitting. The argument `padding_method' is the same as 'mode' in the numpy.pad 
   function (https://numpy.org/doc/stable/reference/generated/numpy.pad.html).


.. py:data:: logger

.. py:class:: PreparedDataset(**args)

   Bases: :py:obj:`data.base_dataset.BaseDataset`


   .. py:method:: __setattr_or_default(args: dict, name: str, default: object)

      Set object attribute with given (if given) or given default. 



   .. py:method:: __prepare()

      Prepare the dataset by splitting and scaling the data.
      Note that this is not done directly on the data. The splits are defined in the
      'selection' variable in a selection matrix. 



   .. py:method:: get_ist_values(set_tag: str)


   .. py:method:: extract_dataset(set_tag: str)

      Extracts the relevant samples for one of the data splits, scale them, and package them. 



   .. py:method:: __fast_extract(the_data: dict, set_tag: str)

      Pad relevant slices. And sample the input values (padded)
      and output values (not padded). 



   .. py:method:: __parr_sample(selection: numpy.array, in_chunk: list, out_chunk: list, max_pad: int, padded_slices: dict, the_data: dict, instances: list, y_map: numpy.array)
      :staticmethod:


      Sample data blocks for parallel processing based on selection indices.

      This function samples data blocks from the provided inputs according to the selection indices. It samples blocks
      from the input data and the target data for parallel processing.

      Parameters:
          selection (array-like): Selection indices specifying the data blocks to sample.
          in_chunk (tuple): Input data chunk size.
          out_chunk (tuple): Target data chunk size.
          max_pad (int): Maximum padding for input data blocks.
          padded_slices (dict): Dictionary containing padded slices of the input data.
          the_data (list): List containing the input data instances.
          instances (list): List of instances related to the input data.
          y_map (array): Mapping for target data components.

      Returns:
          x_vals (array-like): Sampled input data blocks.
          y_vals (array-like): Sampled target data blocks.

      Internal Function:
      sample_blocks(t, chunk, pad):
          Helper function for sampling blocks of data along a given axis.

      The function samples data blocks from the specified inputs based on the selection indices, and returns the
      sampled input and target data as arrays.



   .. py:method:: __do_padding(vals: numpy.array, direction: str, method: str, cap: int)
      :staticmethod:


      Pad vals = [timesteps, features] using the
      given method in the given direction up to the given size (cap).
      See https://numpy.org/doc/stable/reference/generated/numpy.pad.html.



   .. py:method:: __required_prepared_meta()
      :staticmethod:


      These are the variables (and their formats) that need to be given
      when creating a new PreparedDataset object. 



