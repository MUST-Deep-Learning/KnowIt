KnowIt.trainer.base_trainer
===========================

.. py:module:: KnowIt.trainer.base_trainer

.. autoapi-nested-parse::

   -----------
   BaseTrainer
   -----------

   The ``BaseTrainer`` is an abstract class that functions as the interface
   between the context class ``KITrainer`` and any of the concrete trainer state
   objects.

   The ``BaseTrainer`` class stores the user's parameters and defines a set of
   abstract methods to be inherited by the trainer state objects.

   Note that some kwargs in the constructor require additional parameters (for
   example, the learning rate scheduler). In this case, the parameter is always
   provided as a dictionary with the keys as strings specifying the additional
   parameters.

   For example, if one wants to use the `ReduceLROnPlateau` scheduler
   from Pytorch, then one can specify it as a string::

       lr_scheduler = 'ReduceLROnPlateau'

   which will use the default values for the scheduler. However, if one wants to
   use use custom values or the scheduler requires additional kwargs, then the
   scheduler should be passed as a dictionary such as::

       lr_scheduler = {
           'ReduceLROnPlateau':{
               'factor': 0.2,
               'patience': 5,
               'threshold': 0.001
           }
       }
   The same idea holds for any other kwarg in "BaseTrainer" that might need addit-
   ional parameters.



Classes
-------

.. autoapisummary::

   KnowIt.trainer.base_trainer.BaseTrainer


Module Contents
---------------

.. py:class:: BaseTrainer(model, model_params, out_dir, device, loss_fn, optim, max_epochs, learning_rate, lr_scheduler = None, performance_metrics = None, early_stopping_args = None, ckpt_mode = 'min', *, return_final = False, logger_status = False, seed = 123, output_scaler = None)

   Bases: :py:obj:`abc.ABC`


   Abstract class to interface between the context class ``KITrainer`` and
   a trainer state.

   ``BaseTrainer`` will initialize necessary and optional kwargs to be used by
   any of the KnowIt Trainer states. It also defines abstract methods that are
   to be defined in each state object.

   :Parameters: * **model** (:py:class:`Module`) -- The Pytorch model architecture defined by the user in their
                  models directory.
                * **model_params** (:py:class:`dict[str`, :py:class:`Any]`) -- The parameters required to initialize model.
                * **out_dir** (:py:class:`str`) -- The directory to save the model's checkpoint file.
                * **device** (:py:class:`str`) -- The device on which training is to be performed (cpu or gpu).
                * **loss_fn** (:py:class:`str | dict`) -- The loss function to be used during training. The string must
                  match the name in Pytorch's functional library. See:
                  https://pytorch.org/docs/stable/nn.functional.html#loss-functions
                * **optim** (:py:class:`str | dict`) -- The optimizer to be used during training. The string must
                  match the name in Pytorch's `optimizer library <https://pytorch.org/docs/stable/optim.html#algorithms>`_.
                * **max_epochs** (:py:class:`int`) -- The number of training iterations, where a single iteration is
                  over the entire training set.
                * **learning_rate** (:py:class:`float`) -- The learning rate to be used during parameter updates. It
                  controls the size of the updates.
                * **lr_scheduler** (:py:class:`None | str | dict`, *default* :py:obj:`None`) -- The learning rate scheduler to be used during training. If not
                  None, a dictionary must be given of the form::

                      {scheduler: scheduler_kwargs}

                  where:
                      * scheduler: A string that specifies the Pytorch scheduler
                      to be used. Must match names found here:
                      https://pytorch.org/docs/stable/optim.html#module-torch.optim.lr_scheduler

                      * scheduler_kwargs: A dictionary of kwargs required for
                      'scheduler'.
                * **performance_metrics** (:py:class:`None | str | dict`, *default* :py:obj:`None`) -- Performance metrics to be logged during training. If
                  type=dict, then the dictionary must be given of the form::

                      {metric: metric_kwargs}

                  where
                      * metric: A string that specifies the TORCHMETRICS metric
                      to be used. Must match the functional interface names found
                      here: https://lightning.ai/docs/torchmetrics/stable/

                      * metric_kwargs:  A dictionary of kwargs required for
                      'metric'.
                * **early_stopping_args** (:py:class:`None | dict`, *default* :py:obj:`None`) -- Sets the Pytorch Lightning's EarlyStopping callback. If not
                  None, a dictionary must be given with string keywords
                  corresponding to an argument in EarlyStopping and the
                  corresponding value. See:
                  https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.EarlyStopping.html#lightning.pytorch.callbacks.EarlyStopping
                * **ckpt_mode** (:py:class:`str`, *default* ``'min'``) -- Sets the condition for when a model checkpoint should be saved
                  or overwritten during training.
                * **return_final** (:py:class:`bool`, *default* :py:obj:`False`) -- If True, checkpoint file is saved at the end of the last
                  epoch. If False, checkpoint file is saved based on ckpt_mode.
                * **mute_logger** (:py:class:`bool`, *default* :py:obj:`False`) -- If True, the trainer will not log any metrics or save any
                  checkpoints during training.
                * **seed** (:py:class:`None | int`, *default* ``123``) -- If int, sets the random seed value for reproducibility. If
                  None, a new random seed is used for each training run.
                * **output_scaler** (:py:class:`None | object`, *default* :py:obj:`None`) -- The scaling object to rescale the model outputs to original ranges (if applicable)
                  during performance calculations. Must have an appropriate `inverse_transform` function.
                  If None, no rescaling is performed. Note, only applicable to logged metrics,
                  gradients are still calculated with scaled outputs (if applicable).

   :ivar out_dir: Stores the checkpoint file directory.

   :vartype out_dir: :py:class:`str`
   :ivar mute_logger: Sets the activity state (True: on or False: off) of the trainer's
                      metric's logger.

   :vartype mute_logger: :py:class:`bool`
   :ivar seed: Stores the seed value to be used for all random processes.

   :vartype seed: :py:class:`None | int`
   :ivar early_stopping_args: Stores any kwargs required to set up early stopping.

   :vartype early_stopping_args: :py:class:`None | dict`
   :ivar ckpt_mode: Determines the criteria for when a checkpoint should be saved.

   :vartype ckpt_mode: :py:class:`str`
   :ivar return_final: Sets the model checkpointing behaviour when training. If True,
                       checkpoint file is saved at the end of the last epoch. If False,
                       checkpoint file is saved based on ckpt_mode.

   :vartype return_final: :py:class:`bool`
   :ivar pl_model_kwargs: Stores the kwargs required to initialize a Pytorch Lightning model.

   :vartype pl_model_kwargs: :py:class:`dict[str`, :py:class:`Any]`
   :ivar trainer_kwargs: Stores the kwargs required to initialize a Pytorch Lightning's
                         trainer module.

   :vartype trainer_kwargs: :py:class:`dict[str`, :py:class:`Any]`
   :ivar output_scaler: The scaling object to rescale the model outputs to original ranges (if applicable)
                        during performance calculations. Must have an appropriate `inverse_transform` function.
                        If None, no rescaling is performed. Note, only applicable to logged metrics,
                        gradients are still calculated with scaled outputs (if applicable).
   :vartype output_scaler: :py:class:`None | object`, *default* :py:obj:`None`


