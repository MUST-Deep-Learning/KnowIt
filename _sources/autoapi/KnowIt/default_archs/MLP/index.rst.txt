KnowIt.default_archs.MLP
========================

.. py:module:: KnowIt.default_archs.MLP

.. autoapi-nested-parse::

   This is a set of fully connected feed-forward blocks,
   with optional batch normalization and dropout,
   followed by a final linear layer,
   with optional output activation.

   -----------
   HiddenBlock
   -----------

   This block consists of 2 to 4 layers.
   * is optional

   [linear] -> [batch-norm*] -> [activation] -> [dropout*]

       -   [linear] = nn.Linear
       -   [batch-norm*] = nn.BatchNorm1d
       -   [activation] = Depends on the activation function. Set by getattr(nn, activation)(). See https://pytorch.org/docs/stable/nn.html for details.
       -   [dropout*] = nn.Dropout

   .. rubric:: Notes

   - The MLP is capable of handling regression or classification tasks.
   - All HiddenBlocks have bias parameters.
   - All hidden layers have the same number of hidden units, defined by the ``width`` parameter.
   - This architecture flattens the input Tensor, and does not assume any temporal dynamics internally.
   - In practice. The output layer is just a ``HiddenBlock`` with no batch normalization, dropout or bias, and a different activation function.



Classes
-------

.. autoapisummary::

   KnowIt.default_archs.MLP.Model
   KnowIt.default_archs.MLP.HiddenBlock


Module Contents
---------------

.. py:class:: Model(input_dim, output_dim, task_name, *, depth = 3, width = 256, dropout = 0.5, activations = 'ReLU', output_activation = None, batchnorm = True)

   Bases: :py:obj:`torch.nn.Module`


   Defines an MLP architecture.

   The multilayer perceptron (MLP) is a fully connected feedforward neural
   network with nonlinear activation functions.

   For more information on this architecture, see for example:

   [1] "Deep Learning" by I. Goodfellow, Y. Bengio, and A. Courville
   Link: https://www.deeplearningbook.org/

   [2] "Understanding Deep Learning" by S.J.D Prince
   Link: https://udlbook.github.io/udlbook/

   :Parameters: * **input_dim** (:py:class:`list[int]`, :py:class:`shape=[in_chunk`, :py:class:`in_components]`) -- The shape of the input data. The "time axis" is along the first dimension.
                * **output_dim** (:py:class:`list[int]`, :py:class:`shape=[out_chunk`, :py:class:`out_components]`) -- The shape of the output data. The "time axis" is along the first dimension.
                * **task_name** (:py:class:`str`) -- The type of task (classification or regression).
                * **depth** (:py:class:`int`, *default* ``3``) -- The desired number of hidden layers.
                * **width** (:py:class:`int`, *default* ``256``) -- The desired width (number of nodes) of each hidden layer.
                * **dropout** (:py:class:`float | None`, *default* ``0.5``) -- Sets the dropout probability. If None, no dropout is applied.
                * **activations** (:py:class:`str`, *default* ``'ReLU'``) -- Sets the activation type for the hidden units. See https://pytorch.org/docs/stable/nn.html for details.
                * **output_activation** (:py:class:`None | str`, *default* :py:obj:`None`) -- Sets an output activation. See https://pytorch.org/docs/stable/nn.html for details.
                  If None, no output activation.
                * **batchnorm** (:py:class:`bool`, *default* :py:obj:`True`) -- Whether to add batchnorm to hidden layers.

   :ivar task_name: The type of task (classification or regression).
   :vartype task_name: :py:class:`str | None`
   :ivar model_in_dim: Number of model input features. Equal to in_chunk * in_component.
   :vartype model_in_dim: :py:class:`int | None`
   :ivar model_out_dim: Number of model output features. Equal to out_chunk * out_component.
   :vartype model_out_dim: :py:class:`int | None`
   :ivar final_out_dim: The shape of the output data. The "time axis" is along the first dimension.
   :vartype final_out_dim: :py:class:`list[int]`, :py:class:`shape=[out_chunk`, :py:class:`out_components]`
   :ivar model: The entire model architecture.

   :vartype model: :py:class:`nn.Sequential`


   .. py:method:: forward(x)

      Return model output for an input batch.

      :Parameters: **x** (:py:class:`Tensor`, :py:class:`shape=[batch_size`, :py:class:`in_chunk`, :py:class:`in_components]`) -- An input tensor.

      :returns: Model output.
      :rtype: :py:class:`Tensor`, :py:class:`shape=[batch_size`, :py:class:`out_chunk`, :py:class:`out_components]` or :py:class:`[batch_size`, :py:class:`num_classes]`



.. py:class:: HiddenBlock(in_dim, out_dim, *, batchnorm, activation, dropout, bias)

   Bases: :py:obj:`torch.nn.Module`


   A hidden block of an MLP.

   This block consists of a linear layer, followed by an activation function.
   An optional batchnorm layer can be placed before the activation layer,
   and an optional dropout layer can be placed after.

   :Parameters: * **in_dim** (:py:class:`int`) -- The number of input features.
                * **out_dim** (:py:class:`int`) -- The number of output features.
                * **batchnorm** (:py:class:`bool`) -- Whether to include a batchnorm layer before the activation function.
                * **activation** (:py:class:`str | None`) -- The activation function to be used. See https://pytorch.org/docs/stable/nn.html for details.
                * **dropout** (:py:class:`float | None`) -- The dropout probability. If None, no dropout will be applied.
                * **bias** (:py:class:`bool`) -- Whether the linear layer should have a bias vector.

   :ivar block: The hidden block.

   :vartype block: :py:class:`nn.Module`


   .. py:method:: forward(x)

      Applies the hidden block to the input tensor.



